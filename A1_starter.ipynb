{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ling 450/807 SFU - Assignment 1\n",
    "\n",
    "This assignment walks you through two different ways of extracting simple quotes from text and then directs you to a third, already implemented way. Your task is to enhance the simple methods or develop your own. For further instructions, check the assignment file on Canvas. \n",
    "The binder contains this notebook and some sample files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loads and processes only one file at a time. You need to do 5 and comment on the results\n",
    "# to load the 5 texts, you can just change the name of the file below or figure out a way \n",
    "# to pass a list of files to the read command. It's up to you\n",
    "\n",
    "# with open (\"data/5c1dbe1d1e67d78e2797d611.txt\", \"r\", encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# with open (\"data/5c1452701e67d78e276ee126.txt\", \"r\", encoding='utf-8') as f:\n",
    "#     text2 = f.read()\n",
    "\n",
    "# with open (\"data/5c146e42795bd2fcce2ea8e5.txt\", \"r\", encoding='utf-8') as f:\n",
    "#     text3 = f.read()\n",
    "\n",
    "# with open (\"data/5c149ffc1e67d78e276fbd44.txt\", \"r\", encoding='utf-8') as f:\n",
    "#     text4 = f.read()\n",
    "\n",
    "# with open (\"data/5c15488f1e67d78e277161d7.txt\", \"r\", encoding='utf-8') as f:\n",
    "#     text5 = f.read()\n",
    "\n",
    "names = [\"data/5c1dbe1d1e67d78e2797d611.txt\",\"data/5c1dccbf1e67d78e279807d8.txt\",\"data/5c1de1661e67d78e27984d34.txt\",\n",
    "         \"data/5c1df61f1e67d78e2798f3fe.txt\",\"data/5c1e0b68795bd2a5d03a49a9.txt\"]\n",
    "\n",
    "with open (names[0], \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "for t in names[1:]:\n",
    "    with open (t, \"r\", encoding='utf-8') as a:\n",
    "        text += a.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sents(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding text within quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotes(text):\n",
    "    # print(\"{}\\n{}\\n{}\".format(\"allo\", text, \"allo\"))\n",
    "    # quotes = re.findall(r' \"(.*?)\"', text)\n",
    "    # quotes = re.findall(r' [\"«‘‚“](.*?)[\"»’‛”]', text)\n",
    "    # quotes = re.findall(r' «(.*?)»', text)\n",
    "    quotes = re.findall(r' [“‟\"](.*?)[\"””]', text)\n",
    "    \n",
    "    return(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_sents = find_sents(text)\n",
    "# found_sents2 = find_sents(text2)\n",
    "# found_sents3 = find_sents(text3)\n",
    "# found_sents4 = find_sents(text4)\n",
    "# found_sents5 = find_sents(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Honestly, it feels like we're living our worst nightmare right now,\"]\n",
      "['I have no idea where that information came from because both Clark and I were there in the office with all of the workers from the orphanage.']\n",
      "['the Government of Canada has obligations under international conventions to ensure children are not abducted, bought or sold, or removed from their biological families without legal consent.']\n",
      "['in some cases, extra steps in the citizenship or immigration process may be needed to make sure the adoption meets all requirements of international adoption.']\n",
      "[\"We're not giving up, but it feels really overwhelming to think about what this means and what they're trying to do to us right now,\"]\n",
      "['Every publisher rejected me,']\n",
      "['One!']\n",
      "['chicken']\n",
      "['debate']\n",
      "['will have a positive aspirational aspect to it.']\n",
      "['illegal border crossers']\n",
      "['Canadians clearly rejected Stephen Harper’s divisive approach in the last election, which is the same approach the Conservatives are relying on now,']\n",
      "['people’s house,']\n",
      "['has anyone ever considered whether other designs, such as the horseshoe shape used in Australia and Scotland, might work better?']\n"
     ]
    }
   ],
   "source": [
    "# note: this just prints the text in quotes. If you want to save it locally\n",
    "# to analyze how the 3 approaches are different, you need to run a command to save\n",
    "# for instance to a text file\n",
    "with open('./results.txt', 'w') as f:\n",
    "\n",
    "    for sent in found_sents:\n",
    "        str_sent = str(sent)\n",
    "        found_quotes = get_quotes(str_sent)\n",
    "        if len(found_quotes) > 0:\n",
    "            print(found_quotes)\n",
    "            f.write(str(found_quotes) + \"\\n\")\n",
    "\n",
    "    # print('hi')\n",
    "\n",
    "    # for sent in found_sents2:\n",
    "    #     str_sent = str(sent)\n",
    "    #     found_quotes = get_quotes(str_sent)\n",
    "    #     if len(found_quotes) > 0:\n",
    "    #         print(found_quotes)\n",
    "    #         f.write(str(found_quotes) + \"\\n\")\n",
    "\n",
    "    # print('hi2')\n",
    "\n",
    "    # for sent in found_sents3:\n",
    "    #     str_sent = str(sent)\n",
    "    #     found_quotes = get_quotes(str_sent)\n",
    "    #     if len(found_quotes) > 0:\n",
    "    #         print(found_quotes)\n",
    "    #         f.write(str(found_quotes) + \"\\n\")\n",
    "\n",
    "    # print('hi3')\n",
    "\n",
    "    # for sent in found_sents4:\n",
    "    #     str_sent = str(sent)\n",
    "    #     found_quotes = get_quotes(str_sent)\n",
    "    #     if len(found_quotes) > 0:\n",
    "    #         print(found_quotes)\n",
    "    #         f.write(str(found_quotes) + \"\\n\")\n",
    "\n",
    "    # print('hi4')\n",
    "\n",
    "    # for sent in found_sents5:\n",
    "    #     str_sent = str(sent)\n",
    "    #     found_quotes = get_quotes(str_sent)\n",
    "    #     if len(found_quotes) > 0:\n",
    "    #         print(found_quotes)\n",
    "    #         f.write(str(found_quotes) + \"\\n\")\n",
    "\n",
    "# with open('./results.txt', 'w') as f:\n",
    "#     f.write(found_quotes)\n",
    "\n",
    "# for quote in found_quotes:\n",
    "#     with open('./results.txt', 'a') as f:\n",
    "#         f.write(quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Using spaCy's Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is based on notebooks by Dr. W.J.B. Mattingly, http://spacy.pythonhumanities.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the stuff we'll need\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy's Matcher\n",
    "This notebook relies on spaCy's Matcher (see Advanced NLP with spaCy, [chapter 2](https://course.spacy.io/en/chapter2)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding quotes and speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load a text file. Remember, you have to do 5\n",
    "# with open (\"data/5c1dbe1d1e67d78e2797d611.txt\", \"r\", encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "\n",
    "names = [\"data/5c1dbe1d1e67d78e2797d611.txt\",\"data/5c1dccbf1e67d78e279807d8.txt\",\"data/5c1de1661e67d78e27984d34.txt\",\n",
    "         \"data/5c1df61f1e67d78e2798f3fe.txt\",\"data/5c1e0b68795bd2a5d03a49a9.txt\"]\n",
    "\n",
    "with open (names[0], \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "for t in names[1:]:\n",
    "    with open (t, \"r\", encoding='utf-8') as a:\n",
    "        text += a.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to a spacy doc\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396\n",
      "(3232560085755078826, 1, 2) CTV\n",
      "(3232560085755078826, 2, 3) Vancouver\n",
      "(3232560085755078826, 6, 7) Abbotsford\n",
      "(3232560085755078826, 8, 9) B.C.\n",
      "(3232560085755078826, 25, 26) Africa\n",
      "(3232560085755078826, 42, 43) Kim\n",
      "(3232560085755078826, 44, 45) Clark\n",
      "(3232560085755078826, 45, 46) Moran\n",
      "(3232560085755078826, 52, 53) Immigration\n",
      "(3232560085755078826, 54, 55) Refugees\n"
     ]
    }
   ],
   "source": [
    "# This is optional. It just tells you who are the people mentioned. You can use it later if you want to find out the speakers of the quotes\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_n = [{\"POS\": \"PROPN\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "## You can try to extract full names by adding multi-word nouns, http://spacy.pythonhumanities.com/02_02_matcher.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "(16432004385153140588, 1211, 1218) “Every publisher rejected me,”\n",
      "(16432004385153140588, 1337, 1354) “I wanted to emphasize that no matter who you are you can do anything.”\n",
      "(16432004385153140588, 1413, 1420) “Now I have tons.”\n",
      "(16432004385153140588, 1531, 1535) “One!”\n",
      "(16432004385153140588, 1757, 1765) “You gotta start somewhere,”\n",
      "(16432004385153140588, 2073, 2076) “chicken”\n",
      "(16432004385153140588, 2135, 2138) “debate”\n",
      "(16432004385153140588, 2210, 2221) “will have a positive aspirational aspect to it.”\n",
      "(16432004385153140588, 2289, 2294) “illegal border crossers”\n",
      "(16432004385153140588, 4773, 4784) “Why would I pander to the millionaires club?”\n"
     ]
    }
   ],
   "source": [
    "# a simple pattern to extract things in single quotes\n",
    "# as with Approach 1, the for loop prints the results to the screen\n",
    "# you can try and save it to a file if you want to compare with Approach 1 and 3\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "# pattern_q = [{'ORTH': '\"', \"ORTH\":\"“\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"', \"ORTH\": \"”\"}]\n",
    "# matcher.add(\"QUOTES\", [pattern_q], greedy='LONGEST')\n",
    "matcher.add(\"QUOTES\", [pattern_q], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3: Implemented version\n",
    "This approach was implemented by colleagues at the [Australian Text Analytics Platform](https://www.atap.edu.au/) (ATAP). The approach is based on the [Gender Gap Tracker](https://github.com/sfu-discourse-lab/GenderGapTracker) done in the Discourse Processing Lab here at SFU. \n",
    "\n",
    "The first link below leads you to a binder where you can load your own files and download the output. If you prefer to do everything in your own notebook, you can download/clone the project and you'll see a notebook there (quote_extractor_notebook.ipynb)\n",
    "\n",
    "* [Binder link](https://github.com/Australian-Text-Analytics-Platform/quotation-tool/blob/workshop_01_20220908/README.md)\n",
    "* [Regular GitHub project](https://github.com/Australian-Text-Analytics-Platform/quotation-tool)\n",
    "\n",
    "Within the ATAP binder, upload 5 files from A1/data (the same you did for approaches 1 and 2), process them and download the results to your own computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "Check instructions on Canvas for what to do and what to submit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9156c6cfebdd4a7c774b64cfd57e4e760684d036775ff92cac4621bc3969ba1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
